import numpy as np
import cupy as cp
import h5py
from numpy import sum, sqrt
from numpy.random import standard_normal, uniform

from scipy import signal
import os
from tqdm import tqdm
from contextlib import contextmanager
import pickle

@contextmanager
def opened_w_error(filename, mode="r"):
    try:
        f = open(filename, mode)
    except IOError as err:
        yield None, err
    else:
        try:
            yield f, None
        finally:
            f.close()

class Load5gDataset():
    def __init__(self, feature_type='ofdm_absolutes'):
        self.dataset_name = 'data'
        self.labelset_name = 'label'
        assert feature_type in ['ofdm_absolutes', 'obfuscation']
        self.feature_type = feature_type

    def load_channel_estimates(self, file_path, label_list=[], test_to_all_ratio=0.2, n_prbs=273, n_orus=1, n_rx_ant_per_oru = 4, n_tx_ant=1, n_dmrs_symbols = 3, random_subsampling=True, take_middle=False):
        test_csi_list = []
        test_label_list = []
        training_csi_list = []
        training_label_list = []
        for label_idx, label in enumerate(label_list):
            print("Load data for " + label)
            data_path = os.path.join(file_path, label)
            data_files = [f for f in os.listdir(data_path) if os.path.isfile(os.path.join(data_path, f)) and ".pickle" in f]
            n_data_samples = len(data_files)
            H = np.zeros((n_data_samples, n_orus, n_rx_ant_per_oru, n_tx_ant, n_prbs*12, n_dmrs_symbols), dtype=np.complex64)
            noise_var = np.zeros((n_data_samples,n_orus), dtype=np.float32)
            sample_timestamps = np.zeros((n_data_samples), dtype=np.float64)

            print("Processing CSI estimates")

            t = tqdm(total=n_data_samples)
            # read in all pickle files and store data to associated variables
            for idx, data_file in enumerate(data_files):
                t.update()  
                with opened_w_error(os.path.join(data_path, data_file), "rb") as (file, err):
                    if err:
                        print("File " + data_file + " has IO Error: " + str(err))
                    else:
                        x = pickle.load(file)
                        # h_sample = x['ch_est']
                        h_sample = x['ch_est'] # N_ORU x Rx ant x layer x frequency x time
                        H[idx, :, :, :, :, :] = np.squeeze(np.array(h_sample), axis=1)    # np.array should make a copy, slice assignment should also do the copy on itself
                        # H[idx, :, :, :, :, :] = np.take(np.squeeze(np.array(h_sample), axis=1), indices=[0,2], axis=0)    
                        # noise_var[idx, 0] = x['noise_var_dB'][0] # @TODO: here is a bug in the PyAerial Notebook! It should be the noise var from two O-RUs
                        noise_var[idx, :] = np.squeeze(np.array(x['noise_var_dB']))
                        # noise_var[idx, :] = np.take(np.squeeze(np.array(x['noise_var_dB'])), indices=[0,2], axis=0)    
                        # we also have in x the keys 'start_prb', 'num_prbs' but we use all 273 PRBs all of the times

                        timestamp_str = data_file.split("_")[0]
                        timestamp = np.fromstring(timestamp_str, dtype=np.float64, sep='.')
                        sample_timestamps[idx] = timestamp[0]
            
            # compute features
            if self.feature_type == 'ofdm_absolutes':
                print(f"Computing mean absolutes over N_tx={n_tx_ant} and all {n_dmrs_symbols} DMRS symbols")
                H = np.mean(np.abs(H), axis=(3,5))

                print("Normalize per AP")
                H = H / np.linalg.norm(H, ord="fro", axis=(2,3), keepdims=True)

                print("Stack APs for each CSI sample")
                H = np.reshape(H, [n_data_samples, -1, n_prbs*12, 1])

                features = H

            elif self.feature_type == 'obfuscation':
                # H is [n_data_samples, n_orus, n_rx_ant_per_oru, n_tx_ant, n_prbs*12, n_dmrs_symbols]
                H_ = np.transpose(H, [0,3,4,5,1,2])
                # H_ is [n_data_samples, n_tx_ant, n_prbs*12, n_dmrs_symbols, n_orus, n_rx_ant_per_oru]
                H_ = np.reshape(H_, [n_data_samples, n_tx_ant*n_prbs*12*n_dmrs_symbols, -1 ])
                # H_ is [n_data_samples, n_tx_ant * n_prbs * 12 * n_dmrs_symbols, n_orus * n_rx_ant_per_oru]

                # H_ = np.abs(H_)

                print('Normalize each channel vector')
                H_ = H_ / np.linalg.norm(H_, axis=-1, keepdims=True)

                # # Gram and Eigh (for eigenvalue decomposition, computationally more intensive than SVD)
                # R_hat = H_ @ H_.H
                # r_eigenvalues, r_eigenvectors = np.linalg.eigh(R_hat)

                print("Compute SVD")
                #U, S, Vh = np.linalg.svd(H_, full_matrices=False, compute_uv=True, hermitian=False)
                U, S, Vh = cp.linalg.svd(cp.array(H_), full_matrices=False, compute_uv=True)

                principal_singular_vector = np.take(U.get(), indices=0, axis=-1)

                print("Stack real and imaginary part in last dimension")
                features = np.expand_dims(principal_singular_vector, axis=-1)
                features = np.concatenate([np.real(features),np.imag(features)], axis=-1)
                # principal_singular_vector is [n_data_samples, n_tx_ant * n_prbs * 12 * n_dmrs_symbols * 2]
                features = np.reshape(features, [n_data_samples, n_tx_ant * n_prbs * 12, n_dmrs_symbols, 2])

            mempool = cp.get_default_memory_pool()
            mempool.free_all_blocks()

            if random_subsampling:
                print("Randomly shuffle samples")
                np.random.shuffle(features)
            else:
                print("Sort samples according to timestamps")
                time_asc_idx = np.argsort(sample_timestamps)
                features = np.take(features, time_asc_idx, axis=0)

            n_test = round(n_data_samples * test_to_all_ratio)
            n_training = n_data_samples - n_test

            print(f"Append CSI samples from {label} to list")
            if take_middle:
                val_idx = np.arange(n_data_samples//2 - int(np.floor(n_test/2)), n_data_samples//2 + int(np.ceil(n_test/2)))
                tr_idx = np.delete(np.arange(n_data_samples), val_idx)
            else:
                tr_idx = np.arange(0,n_training)
                val_idx = np.arange(n_training,n_data_samples)
            assert np.size(val_idx) == n_test and np.size(tr_idx) == n_training
            training_csi_list.append(np.take(features, indices=tr_idx, axis=0))
            test_csi_list.append(np.take(features, indices=val_idx, axis=0))
            test_label_list.append([label_idx]*n_test)
            training_label_list.append([label_idx]*n_training)

        print("Concatenate all classes")
        training_csi = np.concatenate(training_csi_list)
        training_labels = np.concatenate(training_label_list)
        test_csi = np.concatenate(test_csi_list)
        test_labels = np.concatenate(test_label_list)

        print("Shuffle training data set")
        training_perm_idx = np.arange(np.shape(training_csi)[0])
        np.random.shuffle(training_perm_idx)
        training_csi = np.take(training_csi, indices=training_perm_idx, axis=0)
        training_labels = np.take(training_labels, indices=training_perm_idx, axis=0)

        print("Shuffle test data set")
        test_perm_idx = np.arange(np.shape(test_csi)[0])
        np.random.shuffle(test_perm_idx)
        test_csi = np.take(test_csi, indices=test_perm_idx, axis=0)
        test_labels = np.take(test_labels, indices=test_perm_idx, axis=0)

        return [training_csi, training_labels, test_csi, test_labels]

        

